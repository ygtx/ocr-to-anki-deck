import os
import time
import pathlib
import tempfile
from typing import List, Tuple, Set, Dict, Any
import yt_dlp
from moviepy.editor import VideoFileClip
from PIL import Image
import numpy as np
import cv2
from ..common.ocr import ocr_and_process, ocr_and_process_youtube_frame
from ..common.audio import gen_audio
from .image_table import build_deck
import openai
from skimage.metrics import structural_similarity as ssim
from deep_translator import MyMemoryTranslator
from pathlib import Path
from .base import BaseDeckBuilder
import csv
import base64
import json
import re

def download_video(url: str, output_dir: pathlib.Path) -> pathlib.Path:
    """YouTubeå‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    print(f"\nğŸ“¥ å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {url}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’data/input/youtube/ã«å¤‰æ›´
    youtube_dir = pathlib.Path("data/input/youtube")
    youtube_dir.mkdir(parents=True, exist_ok=True)
    
    ydl_opts = {
        'format': 'best[height<=720]',  # 720pä»¥ä¸‹ã«åˆ¶é™
        'outtmpl': str(youtube_dir / '%(id)s.%(ext)s'),
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        video_path = youtube_dir / f"{info['id']}.{info['ext']}"
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {video_path}")
        return video_path

def extract_frames(video_path: pathlib.Path, output_dir: pathlib.Path, interval: int = 5) -> List[pathlib.Path]:
    """å‹•ç”»ã‹ã‚‰ä¸€å®šé–“éš”ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã™ã‚‹"""
    print(f"\nğŸï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹: {interval}ç§’é–“éš”")
    
    clip = VideoFileClip(str(video_path))
    frame_paths = []
    
    for t in range(0, int(clip.duration), interval):
        frame = clip.get_frame(t)
        frame_path = output_dir / f"frame_{t:04d}.jpg"
        Image.fromarray(frame).save(frame_path)
        frame_paths.append(frame_path)
        print(f"  - {frame_path.name}")
    
    clip.close()
    print(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {len(frame_paths)}æš")
    return frame_paths

def translate_to_english(japanese_text: str) -> str:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³ã™ã‚‹"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a translator. Translate the given Japanese text to English. Only output the English translation, nothing else."},
                {"role": "user", "content": japanese_text}
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ: {japanese_text}")
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return ""

def detect_text_regions(frame: np.ndarray) -> List[np.ndarray]:
    """ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã‚’æ¤œå‡ºã™ã‚‹"""
    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    
    # äºŒå€¤åŒ–
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    # ãƒã‚¤ã‚ºé™¤å»
    kernel = np.ones((3,3), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    
    # è¼ªéƒ­æ¤œå‡º
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®æŠ½å‡º
    text_regions = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        if w > 50 and h > 20:  # å°ã•ã™ãã‚‹é ˜åŸŸã‚’é™¤å¤–
            text_regions.append(frame[y:y+h, x:x+w])
    
    return text_regions

def is_similar_image(img1: np.ndarray, img2: np.ndarray, threshold: float = 0.95) -> bool:
    img1 = cv2.cvtColor(cv2.resize(img1, (200, 200)), cv2.COLOR_RGB2GRAY)
    img2 = cv2.cvtColor(cv2.resize(img2, (200, 200)), cv2.COLOR_RGB2GRAY)
    score, _ = ssim(img1, img2, full=True)
    return score > threshold

def filter_unique_images(image_paths, threshold=0.95):
    unique = []
    unique_paths = []
    for path in image_paths:
        img = np.array(Image.open(path))
        if not any(is_similar_image(img, uimg, threshold) for uimg in unique):
            unique.append(img)
            unique_paths.append(path)
    return unique_paths

def process_youtube_video(url: str, deck_name: str, generate_media: bool = False, frame_interval: int = 5, ssim_threshold: float = 0.95) -> None:
    """YouTubeå‹•ç”»ã‚’å‡¦ç†ã—ã¦Ankiãƒ‡ãƒƒã‚­ã‚’ç”Ÿæˆã™ã‚‹"""
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    temp_dir = pathlib.Path(tempfile.mkdtemp())
    video_dir = temp_dir / "video"
    frame_dir = temp_dir / "frames"
    media_dir = temp_dir / "media"
    translator = MyMemoryTranslator(source='ja-JP', target='en-GB')
    
    for d in [video_dir, frame_dir, media_dir]:
        d.mkdir(exist_ok=True)
    
    try:
        # å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        video_path = download_video(url, video_dir)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®æŠ½å‡º
        print("\nğŸï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹")
        frame_paths = extract_frames(video_path, frame_dir, frame_interval)
        print(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {len(frame_paths)}æš")

        # SSIMã«ã‚ˆã‚‹é‡è¤‡æ’é™¤
        print("\nğŸ” SSIMã«ã‚ˆã‚‹é‡è¤‡æ’é™¤")
        unique_paths = filter_unique_images(frame_paths, threshold=ssim_threshold)
        print(f"âœ… é‡è¤‡æ’é™¤å¾Œ: {len(unique_paths)}æš")

        # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®æ¤œå‡ºãƒ»OCRå‡¦ç†ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”»åƒã®ã¿å¯¾è±¡
        processed_frames = unique_paths

        # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’OCRå‡¦ç†
        all_rows = []
        for frame in processed_frames:
            print(f"\nğŸ“ å‡¦ç†ä¸­: {frame.name}")
            rows = ocr_and_process_youtube_frame(frame, media_dir)
            translated_rows = []
            for meaning, thai, paiboon in rows:
                # æ„å‘³ï¼ˆæ—¥æœ¬èªï¼‰ã‚’è‹±è¨³ï¼ˆdeep-translatorã‚’åˆ©ç”¨ï¼‰
                eng = meaning
                if meaning.strip():
                    try:
                        eng_trans = translator.translate(meaning)
                        if eng_trans:
                            print(f"  ğŸ”„ æ„å‘³ç¿»è¨³: {meaning} â†’ {eng_trans}")
                            eng = eng_trans
                        else:
                            print(f"  âš ï¸ æ„å‘³ç¿»è¨³å¤±æ•—: {meaning}")
                    except Exception as e:
                        print(f"âš ï¸ MyMemoryç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ: {meaning}")
                        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¿ã‚¤èªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆgTTSã¯ç„¡æ–™ãªã®ã§ãã®ã¾ã¾ï¼‰
                audio_file = ""
                try:
                    audio_file = gen_audio(eng, thai, media_dir)
                    time.sleep(0.7)  # gTTSå¯¾ç­–
                except Exception as e:
                    print(f"âš ï¸ éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {thai}")
                    print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                pic_file = ""  # ç”»åƒã¯ä½¿ã‚ãªã„
                translated_rows.append((eng, thai, paiboon, audio_file, pic_file))
            all_rows.extend(translated_rows)
            time.sleep(2.5)  # OpenAI Vision APIå¯¾ç­–
        
        # Paiboonã§é‡è¤‡æ’é™¤
        unique_rows = []
        seen_paiboon = set()
        for eng, thai, paiboon, audio_file, pic_file in all_rows:
            if not paiboon or paiboon in seen_paiboon:
                continue
            seen_paiboon.add(paiboon)
            unique_rows.append((eng, thai, paiboon, audio_file, pic_file))
        
        # ãƒ‡ãƒƒã‚­ã®ç”Ÿæˆ
        build_deck(unique_rows, deck_name, media_dir)
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir)
        print(f"\nğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {temp_dir}")

class YouTubeDeckBuilder(BaseDeckBuilder):
    def __init__(self, output_dir: str, deck_name: str, ssim_threshold: float = 0.95, use_paiboon_correction: bool = True):
        super().__init__(output_dir, deck_name, use_paiboon_correction=use_paiboon_correction)
        self.ssim_threshold = ssim_threshold

    def _extract_frames(self, video_path: Path, interval: int = 1) -> List[Path]:
        """å‹•ç”»ã‹ã‚‰ä¸€å®šé–“éš”ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º"""
        frames = []
        cap = cv2.VideoCapture(str(video_path))
        frame_count = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % interval == 0:
                frame_path = self.temp_dir / f"frame_{frame_count:04d}.jpg"
                cv2.imwrite(str(frame_path), frame)
                frames.append(frame_path)
            
            frame_count += 1
        
        cap.release()
        return frames

    def _remove_duplicates(self, frames: List[Path]) -> List[Path]:
        """SSIMã‚’ä½¿ã£ã¦é‡è¤‡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™¤å»"""
        unique_frames = [frames[0]]
        
        for i in range(1, len(frames)):
            current = cv2.imread(str(frames[i]))
            prev = cv2.imread(str(unique_frames[-1]))
            
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
            current_gray = cv2.cvtColor(current, cv2.COLOR_BGR2GRAY)
            prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)
            
            # SSIMã‚’è¨ˆç®—
            score = ssim(current_gray, prev_gray)
            
            if score < self.ssim_threshold:
                unique_frames.append(frames[i])
        
        return unique_frames

    def _ocr_frame(self, frame_path: Path) -> Dict[str, str]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        # ç”»åƒã‚’èª­ã¿è¾¼ã¿
        with open(frame_path, "rb") as f:
            image_data = f.read()
        b64_image = base64.b64encode(image_data).decode("utf-8")
        # OCRãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‹•çš„ç”Ÿæˆ
        prompt = build_ocr_prompt()
        # OpenAI Vision APIã§OCR
        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{b64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=300
        )
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        result = response.choices[0].message.content
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ã—ã€JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        json_match = re.search(r'```json\n(.*?)\n```', result, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_match = re.search(r'\[[\s\S]*\]', result)
            if not json_match:
                print("âŒ OpenAIå¿œç­”ã«JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {}
            json_str = json_match.group(0)
        try:
            data = json.loads(json_str)
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not isinstance(data, list):
                print("âŒ JSONãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                return {}
            if not data:
                print("âŒ ç©ºã®é…åˆ—ãŒè¿”ã•ã‚Œã¾ã—ãŸ")
                return {}
            # æœ€åˆã®è¦ç´ ã‚’è¿”ã™ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®1ã¤ã ã‘ï¼‰
            item = data[0]
            if not all(k in item for k in ["meaning", "thai", "paiboon"]):
                print("âŒ å¿…è¦ãªã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return {}
            return item
        except Exception as e:
            print(f"âŒ JSONã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return {}

    def build(self, video_path: Path, frame_interval: int = 1) -> Path:
        """å‹•ç”»ã‹ã‚‰ãƒ‡ãƒƒã‚­ã‚’ãƒ“ãƒ«ãƒ‰"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º
        frames = self._extract_frames(video_path, frame_interval)
        
        # é‡è¤‡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™¤å»
        unique_frames = self._remove_duplicates(frames)
        
        # OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        ocr_data = []
        for frame in unique_frames:
            result = self._ocr_frame(frame)
            if result:  # æœ‰åŠ¹ãªçµæœã®å ´åˆã®ã¿è¿½åŠ 
                ocr_data.append(result)
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®buildãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ï¼ˆä¿®æ­£æ©Ÿèƒ½ã‚’å«ã‚€ï¼‰
        return super().build(ocr_data)

    def cleanup(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        super().cleanup()

def build_ocr_prompt():
    diff_path = "data/output/system/paiboon_diff.tsv"
    mis_list = []
    if Path(diff_path).exists():
        with open(diff_path, encoding="utf-8") as f:
            reader = csv.DictReader(f, delimiter="\t")
            for row in reader:
                if row["type"] == "mismatch":
                    mis_list.append(f"- ã‚¿ã‚¤èª: {row['thai']}, æ­£ã—ã„Paiboon: {row['gold_paiboon']}, èª¤åˆ¤å®š: {row['generated_paiboon']}")
    mis_text = "\n".join(mis_list)
    prompt = (
        "ã“ã®ç”»åƒã‹ã‚‰ã‚¿ã‚¤èªã€Paiboonå¼ãƒ­ãƒ¼ãƒå­—ã€æ—¥æœ¬èªã®æ„å‘³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n"
        "å¿…ãšä»¥ä¸‹ã®å½¢å¼ã®JSONé…åˆ—ã§è¿”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã¯ä¸è¦ï¼‰:\n"
        "[\n"
        "  {\n"
        '    "meaning": "æ—¥æœ¬èªã®æ„å‘³",\n'
        '    "thai": "ã‚¿ã‚¤èª",\n'
        '    "paiboon": "Paiboonå¼ãƒ­ãƒ¼ãƒå­—"\n'
        "  }\n"
        "]\n"
        "\nã€é‡è¦ãªæ³¨æ„äº‹é …ã€‘\n"
        "1. å¿…ãšä¸Šè¨˜ã®å½¢å¼ã®JSONé…åˆ—ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚\n"
        "2. éå»ã®OCRå‡¦ç†ã§ã¯ã€Paiboonå¼ãƒ­ãƒ¼ãƒå­—ã®æŠ½å‡ºã«ãŠã„ã¦ä¸‹è¨˜ã®ã‚ˆã†ãªèª¤åˆ¤å®šãŒç¹°ã‚Šè¿”ã—ç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚"
        "ã“ã‚Œã‚‰ã®èª¤ã‚Šã‚’ç¹°ã‚Šè¿”ã•ãªã„ã‚ˆã†ã€ã‚¿ã‚¤èªã®ç™ºéŸ³ãƒ»ç¶´ã‚Šã«å¿ å®ŸãªPaiboonå¼ãƒ­ãƒ¼ãƒå­—ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
        "ç‰¹ã«ã€Paiboonå¼ãƒ­ãƒ¼ãƒå­—ä»¥å¤–ã®è¨˜å·ã‚„æ›–æ˜§ãªæ¨æ¸¬ã«ã‚ˆã‚‹æ–‡å­—ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"
        "\n---\nèª¤åˆ¤å®šä¾‹:\n" + mis_text + "\n---\n"
        "ä¸Šè¨˜ã®èª¤ã‚Šã‚’å‚è€ƒã«ã€åŒã˜é–“é•ã„ã‚’ç¹°ã‚Šè¿”ã•ãšã€æ­£ã—ã„Paiboonå¼ãƒ­ãƒ¼ãƒå­—ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )
    return prompt 