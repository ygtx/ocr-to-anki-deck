import os
import time
import pathlib
import tempfile
from typing import List, Tuple, Set, Dict, Any
import yt_dlp
from moviepy.editor import VideoFileClip
from PIL import Image
import numpy as np
import cv2
from ..common.ocr import ocr_and_process, ocr_and_process_youtube_frame
from ..common.audio import gen_audio
from .image_table import build_deck
import openai
from skimage.metrics import structural_similarity as ssim
from deep_translator import MyMemoryTranslator
from pathlib import Path
from .base import BaseDeckBuilder

def download_video(url: str, output_dir: pathlib.Path) -> pathlib.Path:
    """YouTubeå‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    print(f"\nğŸ“¥ å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {url}")
    
    ydl_opts = {
        'format': 'best[height<=720]',  # 720pä»¥ä¸‹ã«åˆ¶é™
        'outtmpl': str(output_dir / '%(id)s.%(ext)s'),
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        video_path = output_dir / f"{info['id']}.{info['ext']}"
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {video_path}")
        return video_path

def extract_frames(video_path: pathlib.Path, output_dir: pathlib.Path, interval: int = 5) -> List[pathlib.Path]:
    """å‹•ç”»ã‹ã‚‰ä¸€å®šé–“éš”ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã™ã‚‹"""
    print(f"\nğŸï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹: {interval}ç§’é–“éš”")
    
    clip = VideoFileClip(str(video_path))
    frame_paths = []
    
    for t in range(0, int(clip.duration), interval):
        frame = clip.get_frame(t)
        frame_path = output_dir / f"frame_{t:04d}.jpg"
        Image.fromarray(frame).save(frame_path)
        frame_paths.append(frame_path)
        print(f"  - {frame_path.name}")
    
    clip.close()
    print(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {len(frame_paths)}æš")
    return frame_paths

def translate_to_english(japanese_text: str) -> str:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³ã™ã‚‹"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a translator. Translate the given Japanese text to English. Only output the English translation, nothing else."},
                {"role": "user", "content": japanese_text}
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ: {japanese_text}")
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return ""

def detect_text_regions(frame: np.ndarray) -> List[np.ndarray]:
    """ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã‚’æ¤œå‡ºã™ã‚‹"""
    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    
    # äºŒå€¤åŒ–
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    # ãƒã‚¤ã‚ºé™¤å»
    kernel = np.ones((3,3), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    
    # è¼ªéƒ­æ¤œå‡º
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®æŠ½å‡º
    text_regions = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        if w > 50 and h > 20:  # å°ã•ã™ãã‚‹é ˜åŸŸã‚’é™¤å¤–
            text_regions.append(frame[y:y+h, x:x+w])
    
    return text_regions

def is_similar_image(img1: np.ndarray, img2: np.ndarray, threshold: float = 0.95) -> bool:
    img1 = cv2.cvtColor(cv2.resize(img1, (200, 200)), cv2.COLOR_RGB2GRAY)
    img2 = cv2.cvtColor(cv2.resize(img2, (200, 200)), cv2.COLOR_RGB2GRAY)
    score, _ = ssim(img1, img2, full=True)
    return score > threshold

def filter_unique_images(image_paths, threshold=0.95):
    unique = []
    unique_paths = []
    for path in image_paths:
        img = np.array(Image.open(path))
        if not any(is_similar_image(img, uimg, threshold) for uimg in unique):
            unique.append(img)
            unique_paths.append(path)
    return unique_paths

def process_youtube_video(url: str, deck_name: str, generate_media: bool = False, frame_interval: int = 5, ssim_threshold: float = 0.95) -> None:
    """YouTubeå‹•ç”»ã‚’å‡¦ç†ã—ã¦Ankiãƒ‡ãƒƒã‚­ã‚’ç”Ÿæˆã™ã‚‹"""
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    temp_dir = pathlib.Path(tempfile.mkdtemp())
    video_dir = temp_dir / "video"
    frame_dir = temp_dir / "frames"
    media_dir = temp_dir / "media"
    translator = MyMemoryTranslator(source='ja-JP', target='en-GB')
    
    for d in [video_dir, frame_dir, media_dir]:
        d.mkdir(exist_ok=True)
    
    try:
        # å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        video_path = download_video(url, video_dir)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®æŠ½å‡º
        print("\nğŸï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹")
        frame_paths = extract_frames(video_path, frame_dir, frame_interval)
        print(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {len(frame_paths)}æš")

        # SSIMã«ã‚ˆã‚‹é‡è¤‡æ’é™¤
        print("\nğŸ” SSIMã«ã‚ˆã‚‹é‡è¤‡æ’é™¤")
        unique_paths = filter_unique_images(frame_paths, threshold=ssim_threshold)
        print(f"âœ… é‡è¤‡æ’é™¤å¾Œ: {len(unique_paths)}æš")

        # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®æ¤œå‡ºãƒ»OCRå‡¦ç†ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”»åƒã®ã¿å¯¾è±¡
        processed_frames = unique_paths

        # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’OCRå‡¦ç†
        all_rows = []
        for frame in processed_frames:
            print(f"\nğŸ“ å‡¦ç†ä¸­: {frame.name}")
            rows = ocr_and_process_youtube_frame(frame, media_dir)
            translated_rows = []
            for meaning, thai, paiboon in rows:
                # æ„å‘³ï¼ˆæ—¥æœ¬èªï¼‰ã‚’è‹±è¨³ï¼ˆdeep-translatorã‚’åˆ©ç”¨ï¼‰
                eng = meaning
                if meaning.strip():
                    try:
                        eng_trans = translator.translate(meaning)
                        if eng_trans:
                            print(f"  ğŸ”„ æ„å‘³ç¿»è¨³: {meaning} â†’ {eng_trans}")
                            eng = eng_trans
                        else:
                            print(f"  âš ï¸ æ„å‘³ç¿»è¨³å¤±æ•—: {meaning}")
                    except Exception as e:
                        print(f"âš ï¸ MyMemoryç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ: {meaning}")
                        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ã‚¿ã‚¤èªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆgTTSã¯ç„¡æ–™ãªã®ã§ãã®ã¾ã¾ï¼‰
                audio_file = ""
                try:
                    audio_file = gen_audio(eng, thai, media_dir)
                    time.sleep(0.7)  # gTTSå¯¾ç­–
                except Exception as e:
                    print(f"âš ï¸ éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {thai}")
                    print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                pic_file = ""  # ç”»åƒã¯ä½¿ã‚ãªã„
                translated_rows.append((eng, thai, paiboon, audio_file, pic_file))
            all_rows.extend(translated_rows)
            time.sleep(2.5)  # OpenAI Vision APIå¯¾ç­–
        
        # Paiboonã§é‡è¤‡æ’é™¤
        unique_rows = []
        seen_paiboon = set()
        for eng, thai, paiboon, audio_file, pic_file in all_rows:
            if not paiboon or paiboon in seen_paiboon:
                continue
            seen_paiboon.add(paiboon)
            unique_rows.append((eng, thai, paiboon, audio_file, pic_file))
        
        # ãƒ‡ãƒƒã‚­ã®ç”Ÿæˆ
        build_deck(unique_rows, deck_name, media_dir)
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir)
        print(f"\nğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {temp_dir}")

class YouTubeDeckBuilder(BaseDeckBuilder):
    def __init__(self, output_dir: str, deck_name: str, ssim_threshold: float = 0.95):
        super().__init__(output_dir, deck_name)
        self.ssim_threshold = ssim_threshold

    def _extract_frames(self, video_path: Path, interval: int = 1) -> List[Path]:
        """å‹•ç”»ã‹ã‚‰ä¸€å®šé–“éš”ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º"""
        frames = []
        cap = cv2.VideoCapture(str(video_path))
        frame_count = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % interval == 0:
                frame_path = self.temp_dir / f"frame_{frame_count:04d}.jpg"
                cv2.imwrite(str(frame_path), frame)
                frames.append(frame_path)
            
            frame_count += 1
        
        cap.release()
        return frames

    def _remove_duplicates(self, frames: List[Path]) -> List[Path]:
        """SSIMã‚’ä½¿ã£ã¦é‡è¤‡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™¤å»"""
        unique_frames = [frames[0]]
        
        for i in range(1, len(frames)):
            current = cv2.imread(str(frames[i]))
            prev = cv2.imread(str(unique_frames[-1]))
            
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
            current_gray = cv2.cvtColor(current, cv2.COLOR_BGR2GRAY)
            prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)
            
            # SSIMã‚’è¨ˆç®—
            score = ssim(current_gray, prev_gray)
            
            if score < self.ssim_threshold:
                unique_frames.append(frames[i])
        
        return unique_frames

    def _ocr_frame(self, frame_path: Path) -> Dict[str, str]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        # ç”»åƒã‚’èª­ã¿è¾¼ã¿
        with open(frame_path, "rb") as f:
            image_data = f.read()
        
        # OpenAI Vision APIã§OCR
        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "ã“ã®ç”»åƒã‹ã‚‰ã‚¿ã‚¤èªã€Paiboonå¼ãƒ­ãƒ¼ãƒå­—ã€æ—¥æœ¬èªã®æ„å‘³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data.hex()}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=300
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        result = response.choices[0].message.content
        return eval(result)  # JSONæ–‡å­—åˆ—ã‚’Dictã«å¤‰æ›

    def build(self, video_path: Path, frame_interval: int = 1) -> Path:
        """å‹•ç”»ã‹ã‚‰ãƒ‡ãƒƒã‚­ã‚’ãƒ“ãƒ«ãƒ‰"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º
        frames = self._extract_frames(video_path, frame_interval)
        
        # é‡è¤‡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™¤å»
        unique_frames = self._remove_duplicates(frames)
        
        # OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        ocr_data = []
        for frame in unique_frames:
            result = self._ocr_frame(frame)
            if result:  # æœ‰åŠ¹ãªçµæœã®å ´åˆã®ã¿è¿½åŠ 
                ocr_data.append(result)
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®buildãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ï¼ˆä¿®æ­£æ©Ÿèƒ½ã‚’å«ã‚€ï¼‰
        return super().build(ocr_data)

    def cleanup(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        super().cleanup() 